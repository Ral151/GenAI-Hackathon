{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ral151/GenAI-Hackathon/blob/main/MediBot_Version_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avGM4p4GqJkK",
        "outputId": "c41aa38d-2c3c-4c7d-b70f-db8fdc39d7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "Transformers: 4.46.3\n",
            "PEFT: 0.17.1\n",
            "TRL: 0.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip -q install \"transformers>=4.43,<4.47\" \"datasets>=2.19\" \"accelerate>=0.32\" \\\n",
        "                 \"peft>=0.11\" \"trl>=0.9.6\" \"bitsandbytes>=0.43.2\" \\\n",
        "                 einops sentencepiece tiktoken jsonschema\n",
        "\n",
        "import torch, transformers, peft, trl, datasets, bitsandbytes\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"PEFT:\", peft.__version__)\n",
        "print(\"TRL:\", trl.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxoSrTag-d6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54dff0cf-2d1b-41d4-e78c-2de896c013f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.17.0)\n",
            "Collecting trl\n",
            "  Using cached trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Using cached trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: tokenizers, transformers, trl\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.17.0\n",
            "    Uninstalling trl-0.17.0:\n",
            "      Successfully uninstalled trl-0.17.0\n",
            "Successfully installed tokenizers-0.22.1 transformers-4.57.1 trl-0.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers",
                  "trl"
                ]
              },
              "id": "d45a887ddb274ecda937ba49874728ea"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade trl peft transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU3bLCNIg1ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca657c35-a0d6-4722-ad25-e09a7242e7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 24 05:20:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/healthbot:\n",
            "data  models\n",
            "\n",
            "/content/drive/MyDrive/healthbot/data:\n",
            "\n",
            "/content/drive/MyDrive/healthbot/models:\n",
            "qwen2.5-1.5b-triage-admin-lora\n",
            "\n",
            "/content/drive/MyDrive/healthbot/models/qwen2.5-1.5b-triage-admin-lora:\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# project paths  (do NOT change)\n",
        "PROJ = \"/content/drive/MyDrive/healthbot\"\n",
        "DATA_DIR = f\"{PROJ}/data\"\n",
        "MODEL_OUT = f\"{PROJ}/models/qwen2.5-1.5b-triage-admin-lora\"\n",
        "\n",
        "!mkdir -p \"$DATA_DIR\" \"$MODEL_OUT\"\n",
        "!ls -R \"$PROJ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_tJIcfWqO4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616,
          "referenced_widgets": [
            "09e101514e43467aa6bc64e3ee9af860",
            "57b5e19e376e4db8b71dbe7426671be5",
            "3fedc30b127d440e8557e0902ec3d48d",
            "1d8f5032babb488a8045cbb8e348b49b",
            "0dd7592f0ea74b529e1e2ea5ceb370ba",
            "2bc0ba650a814733ac475ac620ef948d",
            "8980b7f6b11f4cb8992efc96c98e12a4",
            "d6f65a66c0a5471a919044bb767571bd",
            "0187406a9ade4143b9d2be9fea761d1b",
            "e34121c5e9ae49f1ae44b1ce2be6b196",
            "11ab7322956849c28422234916ab6481"
          ]
        },
        "outputId": "32b55fae-3310-454c-e9b5-124124c7510c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e101514e43467aa6bc64e3ee9af860"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.qwen2.configuration_qwen2 because of the following error (look up to see its traceback):\ncannot import name 'layer_type_validation' from 'transformers.configuration_utils' (/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mreally\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstarted\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlocally\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfollow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstructions\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/configuration_qwen2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_type_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_rope_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrope_config_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'layer_type_validation' from 'transformers.configuration_utils' (/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2678414553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 2. Load the model with the new quantization_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Note: The torch_dtype is now specified inside the config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mBASE_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0madapter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             maybe_adapter_path = find_adapter_config_file(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_commit_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_LazyConfigMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m     \"\"\"\n\u001b[1;32m   1036\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mlazily\u001b[0m \u001b[0mload\u001b[0m \u001b[0mits\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mrequested\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"nllb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NLLB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"nllb-moe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NLLB-MOE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"nougat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Nougat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"nystromformer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Nystr√∂mformer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"olmo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OLMo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m TORCHVISION_IMPORT_ERROR = \"\"\"\n\u001b[1;32m   1765\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mTorchvision\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mCheck\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstructions\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m \u001b[0minstallation\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstarted\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlocally\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfollow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mones\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mnote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mrestart\u001b[0m \u001b[0myour\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0mafter\u001b[0m \u001b[0minstallation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mreally\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstarted\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlocally\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfollow\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstructions\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m \u001b[0mmatch\u001b[0m \u001b[0myour\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m \"\"\"\n\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.qwen2.configuration_qwen2 because of the following error (look up to see its traceback):\ncannot import name 'layer_type_validation' from 'transformers.configuration_utils' (/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py)"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "MODEL_OUT = \"/content/drive/MyDrive/healthbot/models/qwen2.5-1.5b-triage-admin-lora\"\n",
        "\n",
        "# 1. Define the quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# 2. Load the model with the new quantization_config\n",
        "# Note: The torch_dtype is now specified inside the config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "\n",
        "# You can now proceed with your model\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMtbGg-mrK3b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "dataset_path = os.path.join(DATA_DIR, \"train.jsonl\")\n",
        "if os.path.exists(dataset_path):\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
        "    print(\"‚úÖ Dataset loaded:\", dataset)\n",
        "else:\n",
        "    dataset = None\n",
        "    print(\"‚ö†Ô∏è No dataset found. Skipping fine-tuning step.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfAWv9NlrLf1"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "import transformers\n",
        "from trl import SFTTrainer, SFTConfig # Make sure to import SFTConfig\n",
        "\n",
        "# Ensure your dataset is loaded and not None\n",
        "if 'dataset' in locals() and dataset is not None:\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    messages_column = \"messages\"\n",
        "\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # Create the SFTConfig object, but WITHOUT max_seq_length\n",
        "    sft_config = SFTConfig(\n",
        "        dataset_text_field=messages_column,\n",
        "        max_length=1024,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=5,\n",
        "        output_dir=MODEL_OUT,\n",
        "        save_total_limit=2,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        report_to=\"none\"\n",
        "        # The SFTTrainer handles completion-only loss automatically\n",
        "        # when a response_template is provided, so you often don't need a custom collator.\n",
        "        # We can remove the collator for a simpler setup.\n",
        "    )\n",
        "\n",
        "    # Now, initialize the SFTTrainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        peft_config=peft_config,\n",
        "        args=sft_config, # Pass the config objects\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.model.save_pretrained(MODEL_OUT)\n",
        "    tokenizer.save_pretrained(MODEL_OUT)\n",
        "    print(f\"‚úÖ Fine-tuned model saved to {MODEL_OUT}\")\n",
        "\n",
        "else:\n",
        "    print(\"üí° Skipping fine-tuning because the dataset was not loaded.\")\n",
        "\n",
        "    \"\"\"# This is the correct way\n",
        "    from trl import SFTTrainer\n",
        "    from trl.data import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # the column in your JSON that contains the combined prompt+response text\n",
        "    messages_column = \"messages\"\n",
        "\n",
        "    collator = DataCollatorForCompletionOnlyLM(\n",
        "        response_template=\"### Response:\",\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=messages_column,\n",
        "        max_seq_length=1024,\n",
        "        data_collator=collator,\n",
        "        args=transformers.TrainingArguments(\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=10,\n",
        "            max_steps=100,   # adjust this for longer training\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=5,\n",
        "            output_dir=MODEL_OUT,\n",
        "            save_total_limit=2,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            report_to=\"none\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.model.save_pretrained(MODEL_OUT)\n",
        "    tokenizer.save_pretrained(MODEL_OUT)\n",
        "    print(f\"‚úÖ Fine-tuned model saved to {MODEL_OUT}\")\n",
        "\n",
        "else:\n",
        "    print(\"üí° Skipping fine-tuning because dataset not found.\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uemLmgsora0x"
      },
      "outputs": [],
      "source": [
        "# =========================================================================\n",
        "# PASTE THIS ENTIRE BLOCK INTO A NEW CELL AT THE END OF YOUR NOTEBOOK\n",
        "# =========================================================================\n",
        "\n",
        "# üß© Install dependencies (safe to rerun)\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q git+https://github.com/openai/whisper.git soundfile gTTS\n",
        "!sudo apt-get update -y && sudo apt-get install -y ffmpeg espeak\n",
        "\n",
        "import os, time, tempfile, torch, whisper, psutil\n",
        "import IPython.display as ipd\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from gtts import gTTS\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ Load your fine‚Äëtuned Qwen model (using your smart loader logic)\n",
        "# ============================================================\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/healthbot/models/qwen2.5-1.5b-triage-admin-lora\"\n",
        "\n",
        "print(f\"üöÄ Loading fine‚Äëtuned model from {MODEL_PATH} ‚Ä¶\")\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"Model not found at {MODEL_PATH}. Make sure training completed and saved the model.\")\n",
        "\n",
        "    # Prepare tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "    # Check for GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"‚úÖ GPU detected. Loading model in 4-bit quantized mode.\")\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            quantization_config=bnb_cfg,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        print(\"üíª No GPU detected. Loading model on CPU (will be slower).\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            device_map=\"cpu\", # Explicitly map to CPU\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        device = \"cpu\"\n",
        "\n",
        "    print(f\"‚úÖ Model ready on {device.upper()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR: Could not load the model. Please check paths and GPU/RAM availability.\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    # We will stop here if the model fails to load\n",
        "    model = None\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ Define the AI's persona with a strong System Prompt\n",
        "# ============================================================\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a specialized AI assistant for the 'HealthPoint Clinic' administration desk. Your name is 'AdminHelper'.\n",
        "\n",
        "Your ONLY purpose is to help users with administrative tasks, including:\n",
        "- Booking, rescheduling, or canceling appointments.\n",
        "- Providing information about clinic hours, location, and contact details.\n",
        "- Answering questions about billing and accepted insurance.\n",
        "- Guiding users on how to access their patient portal.\n",
        "\n",
        "**CRITICAL RULES:**\n",
        "1.  **DO NOT PROVIDE MEDICAL ADVICE.** You are not a doctor, nurse, or paramedic.\n",
        "2.  If a user asks for medical advice, describes symptoms (like 'I have a headache'), or asks about treatment, you MUST politely decline and redirect them. Say: \"I cannot provide medical advice. For any medical concerns, please consult with a healthcare professional or contact emergency services if this is an urgent matter.\"\n",
        "3.  Do not answer questions outside of your administrative scope (e.g., weather, news). Politely state that you can only assist with clinic-related administrative tasks.\n",
        "4.  Keep your answers concise and professional.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ Define Audio, STT, and TTS Functions\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "def record_audio(filename=\"voice_input.wav\"):\n",
        "    # This JavaScript is a self-contained, immediately-invoked function (IIFE).\n",
        "    # It's a much more stable way to run JS in Colab.\n",
        "    record_js_iife = \"\"\"\n",
        "    (async function() {\n",
        "      try {\n",
        "        const duration = 5000; // Recording duration in milliseconds\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "        const recorder = new MediaRecorder(stream);\n",
        "        let data = [];\n",
        "\n",
        "        recorder.ondataavailable = e => data.push(e.data);\n",
        "        recorder.start();\n",
        "\n",
        "        console.log(\"üé§ Recording... Speak now for 5 seconds!\");\n",
        "\n",
        "        // Wait for the specified duration\n",
        "        await new Promise(resolve => setTimeout(resolve, duration));\n",
        "\n",
        "        recorder.stop();\n",
        "\n",
        "        // Important: Stop all audio tracks to release the microphone\n",
        "        stream.getTracks().forEach(track => track.stop());\n",
        "\n",
        "        console.log(\"‚è≥ Processing audio...\");\n",
        "\n",
        "        // Wait for the recorder to finish processing the data\n",
        "        await new Promise(resolve => recorder.onstop = resolve);\n",
        "\n",
        "        const blob = new Blob(data, { type: 'audio/wav' });\n",
        "        const arrayBuffer = await blob.arrayBuffer();\n",
        "        // Convert to base64\n",
        "        const base64 = btoa(String.fromCharCode(...new Uint8Array(arrayBuffer)));\n",
        "        return base64;\n",
        "      } catch (e) {\n",
        "        // If any error occurs in the JS, return it as a prefixed string\n",
        "        console.error(\"JavaScript Audio Error:\", e);\n",
        "        return \"JS_ERROR:\" + e.name + \": \" + e.message;\n",
        "      }\n",
        "    })()\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"‚ö° Recording will start. Please allow microphone access if prompted.\")\n",
        "        # Execute the self-contained JavaScript\n",
        "        audio_b64 = output.eval_js(record_js_iife)\n",
        "\n",
        "        # Check if the JavaScript side sent back our specific error string\n",
        "        if isinstance(audio_b64, str) and audio_b64.startswith(\"JS_ERROR:\"):\n",
        "            raise RuntimeError(f\"JavaScript Error -> {audio_b64.replace('JS_ERROR:', '')}\")\n",
        "\n",
        "        if not audio_b64:\n",
        "            raise RuntimeError(\"Audio capture returned empty. Is the microphone silent?\")\n",
        "\n",
        "        # Decode the base64 string and save as a .wav file\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(b64decode(audio_b64))\n",
        "        print(f\"‚úÖ Audio successfully saved to {filename}\")\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        # This will now print the clear error from either Python or JavaScript\n",
        "        print(f\"üî¥ Audio recording failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def transcribe(filepath):\n",
        "    if not filepath: return \"\", \"en\"\n",
        "    print(\"üß† Transcribing...\")\n",
        "    result = stt_model.transcribe(filepath, fp16=torch.cuda.is_available())\n",
        "    text = result[\"text\"].strip()\n",
        "    lang = result[\"language\"]\n",
        "    print(f\"üó£Ô∏è You said: {text} (Language: {lang})\")\n",
        "    return text, lang\n",
        "\n",
        "def speak(text, lang_code=\"en\"):\n",
        "    if not text.strip(): return\n",
        "    try:\n",
        "        tmp_path = os.path.join(tempfile.gettempdir(), \"voice_reply.mp3\")\n",
        "        safe_lang = \"zh\" if lang_code == \"zh\" else \"en\"\n",
        "        print(f\"üîä Speaking reply via gTTS ({safe_lang})...\")\n",
        "        tts_obj = gTTS(text=text, lang=safe_lang, slow=False)\n",
        "        tts_obj.save(tmp_path)\n",
        "        ipd.display(ipd.Audio(tmp_path, autoplay=True))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Text-to-Speech error: {e}\")\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ Update `ask_model` to use conversation history and the system prompt\n",
        "# ============================================================\n",
        "\n",
        "def ask_model(user_prompt, messages_history):\n",
        "    if not user_prompt:\n",
        "        return \"I'm sorry, I didn't catch that. Could you please repeat?\", messages_history\n",
        "\n",
        "    messages_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "    full_prompt = tokenizer.apply_chat_template(messages_history, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
        "    reply_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    final_reply = reply_text[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):].strip()\n",
        "    messages_history.append({\"role\": \"assistant\", \"content\": final_reply})\n",
        "\n",
        "    print(f\"ü§ñ AdminHelper replied: {final_reply}\")\n",
        "    return final_reply, messages_history\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ Create the main continuous chat loop\n",
        "# ============================================================\n",
        "\n",
        "# =====================================================================\n",
        "# CORRECTED: Button-Driven \"Push to Talk\" Chat\n",
        "# This version uses the correct 'ipywidgets' library.\n",
        "# =====================================================================\n",
        "\n",
        "import ipywidgets as widgets  # <--- THIS IS THE CORRECTED LINE\n",
        "import IPython.display\n",
        "\n",
        "# 1. Initialize the conversation history.\n",
        "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "\n",
        "# 2. Define a function that runs ONE conversation turn.\n",
        "def run_one_conversation_turn(b):\n",
        "    IPython.display.clear_output(wait=True)\n",
        "    display(button)\n",
        "    print(\"----------------------------------------------------\")\n",
        "\n",
        "    try:\n",
        "        audio_path = record_audio()\n",
        "        if not audio_path:\n",
        "            print(\"üî¥ Recording failed or was silent. Please check mic permissions and try again.\")\n",
        "            return\n",
        "\n",
        "        user_text, lang_code = transcribe(audio_path)\n",
        "        if not user_text:\n",
        "            print(\"ü§î I didn't hear anything clearly. Please try again.\")\n",
        "            return\n",
        "\n",
        "        global messages\n",
        "        model_reply, messages = ask_model(user_text, messages)\n",
        "        speak(model_reply, lang_code)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the turn: {e}\")\n",
        "\n",
        "# 3. Create the button widget. This will now work correctly.\n",
        "button = widgets.Button(description=\"üé§ Click to Speak\")\n",
        "\n",
        "# 4. Tell the button to call our function whenever it's clicked.\n",
        "button.on_click(run_one_conversation_turn)\n",
        "\n",
        "# 5. Display the initial welcome message and the button.\n",
        "if model is not None:\n",
        "    print(\"‚úÖ AdminHelper is ready.\")\n",
        "    print(\"Click the 'üé§ Click to Speak' button below to start.\")\n",
        "    print(\"====================================================\")\n",
        "    display(button)\n",
        "else:\n",
        "    print(\"üî¥ Model not loaded. Cannot start the chat.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09e101514e43467aa6bc64e3ee9af860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57b5e19e376e4db8b71dbe7426671be5",
              "IPY_MODEL_3fedc30b127d440e8557e0902ec3d48d",
              "IPY_MODEL_1d8f5032babb488a8045cbb8e348b49b"
            ],
            "layout": "IPY_MODEL_0dd7592f0ea74b529e1e2ea5ceb370ba"
          }
        },
        "57b5e19e376e4db8b71dbe7426671be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bc0ba650a814733ac475ac620ef948d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8980b7f6b11f4cb8992efc96c98e12a4",
            "value": "config.json:‚Äá100%"
          }
        },
        "3fedc30b127d440e8557e0902ec3d48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6f65a66c0a5471a919044bb767571bd",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0187406a9ade4143b9d2be9fea761d1b",
            "value": 660
          }
        },
        "1d8f5032babb488a8045cbb8e348b49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e34121c5e9ae49f1ae44b1ce2be6b196",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_11ab7322956849c28422234916ab6481",
            "value": "‚Äá660/660‚Äá[00:00&lt;00:00,‚Äá19.2kB/s]"
          }
        },
        "0dd7592f0ea74b529e1e2ea5ceb370ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bc0ba650a814733ac475ac620ef948d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8980b7f6b11f4cb8992efc96c98e12a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f65a66c0a5471a919044bb767571bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0187406a9ade4143b9d2be9fea761d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e34121c5e9ae49f1ae44b1ce2be6b196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11ab7322956849c28422234916ab6481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}